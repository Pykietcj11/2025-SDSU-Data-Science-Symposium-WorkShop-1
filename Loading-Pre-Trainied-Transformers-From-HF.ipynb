{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2025 SDSU Data Science Symposium**\n",
    "\n",
    "**Venue:** Student Union, South Dakota State University, Brookings, SD, USA\n",
    "\n",
    "**Date of the session:** February 6, 2025\n",
    "\n",
    "**Instructor :** Cameron Pykiet, Jaylin Dyson, Bishnu Sarker\n",
    "\n",
    "**Affiliation :** Meharry Medical College School of Applied Computational Sciences, Tennessee, USA\n",
    "\n",
    "Please cite this tutorial as:\n",
    "\n",
    "**Bishnu Sarker, Cameron Pykiet, Jaylin Dyson (2025, February). Workshop 1: Building Interdisciplinary applications using Large Language Models. In 2025 Data Science Symposium at South Dakota State University, February 6-7, 2025, Brookings, SD.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case study 1: Protein Function Prediction using Generative Large Language Model.**\n",
    "\n",
    "**How to use a pre-trained Transformer Model for generating sequence embeddings**\n",
    "\n",
    "RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks) possess a remarkable capability to model sequential data and capture spatial dependencies. They have been widely utilized as go-to deep learning models for tasks involving sequence data, such as text classification. However, one limitation of traditional LSTMs is their inability to attend to different parts of the input sentence simultaneously.\n",
    "\n",
    "In certain classification tasks, it becomes essential to associate different sections of the input sequence with one another. By doing so, we can unveil important patterns and relationships that describe the connection between the sequence and the corresponding label.\n",
    "\n",
    "To address this limitation, more advanced models and architectures have been developed, such as the attention mechanism. Attention-based models allow for the dynamic allocation of attention weights to different parts of the input sequence, enabling the model to focus on the most relevant and informative elements. By incorporating attention mechanisms, we can enhance the model's ability to capture intricate patterns and dependencies, leading to improved performance in tasks requiring sequence classification.\n",
    "\n",
    "By leveraging attention mechanisms, we can unlock the potential to discover crucial associations between various parts/tokens of the input sequence, thereby enhancing the model's comprehension and representation of the underlying long distance relationships.\n",
    "\n",
    "The researches published in [( Bahdanau et al., 2014)](https://arxiv.org/pdf/1409.0473.pdf) and [( Luong et al., 2015.)](https://arxiv.org/pdf/1508.04025.pdf) introduced attention mechanism  roughly a decade ago. Following that progress, in 2017 a team of googe researchers first developed the Transformer architecture published as [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) that uses multiple attentions (aka multi-head attention). A visual description of the attention mechanism is available [here](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) and a much detail explanation [here](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452). Transformers are a encoder-decoder model designed to perform multitudes of sequence modeling tasks such as classification, translations, learning representaitons etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are resource intensive model in a sense that they require powerful machines and gpus to train. Therefore, it is not feasible to train a new model for every task. There is the concept of pre-training emerges that perform the transfer learning. The model is pre-trained using a resource rich computing facility. The model architecture with pre-trained parameters are shared with the public. The pre-trained models are then used to build other models for many downstream tasks.\n",
    "\n",
    "In this hands-on, the objectives are:\n",
    "1. Load  pre-trained transformer model trained on millions of protein sequences from UniProtKB.\n",
    "2. Generate the embeddings for a list of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up your python environment. \n",
    "To work with large language models, there are a number of python packages that facilitate the implementation. \n",
    "\n",
    "Following command will install:\n",
    "1. torch\n",
    "2. torchvision\n",
    "3. torchaudio\n",
    "4.transformers\n",
    "5.sentencepiece\n",
    "6.accelerate\n",
    "\n",
    "\n",
    "!pip3 install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "it is highly recommended that you setup a virtual environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading a pre-trained model. \n",
    "As it is stated in [UniProtKB](https://www.uniprot.org/help/embeddings), \"Protein embeddings are a way to encode functional and structural properties of a protein, mostly from its sequence only, in a machine-friendly format (vector representation). Generating such embeddings is computationally expensive, but once computed they can be leveraged for different tasks, such as sequence similarity search, sequence clustering, and sequence classification.\"\n",
    "In an attempt to build language model for protein sequences, [ROST LAB](https://www.rostlab.org/) released [ProTrans](https://github.com/agemagician/ProtTrans) published [here](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3).\n",
    "Using the ProtTrans T5 model, UniProtKB generated 1024 dimension long  [Embeddings](https://www.uniprot.org/help/downloads#embeddings) for all of the reviewed sequences in SwissProt database.\n",
    "A detailed description of the inner working of the model can be found in the paper and github repository mentioned above.\n",
    "\n",
    "In the recent days, a number of generative protein language models were released and shared in hugging face such as `ProtGPT2 (nferruz/ProtGPT2)`, `ProLLaMA(GreatCaptainNemo/ProLLaMA)`, `ESM-2(facebook/esm2_t33_650M_UR50D)`. This is an ongoing trend and fast moving area of research and development. We expect to see more powerful protein language models will be emerging and shared publicly. The primary purpose was to generate protein sequences based on property as well as predicting property based on protein sequence. \n",
    "\n",
    "In this section, the learning objectives would be:\n",
    "1. To learn the syntax for loading pre-trained language model. \n",
    "2. To extract embedding from pre-trained language model. \n",
    "3. To programmatically access the generative language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EsmModel, EsmTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the model and extracting embeddings per sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ProtGPT2**\n",
    "\n",
    "ProtGPT2 is a generative protein language model based on GPT-2, optimized for creating realistic protein-like sequences. It has applications in protein design and exploring protein sequence space. ProtGPT2(https://pmc.ncbi.nlm.nih.gov/articles/PMC9329459/) utilized a Transformer decoder model architecture, processing input sequences tokenized through a `Byte Pair Encoding (BPE)` strategy. During training, the model employed the original dot-product self-attention mechanism. This architecture consists of 36 layers with a model dimensionality of `1280`, aligning with the GPT-2 large Transformer architecture obtained from HuggingFace. Prior to training, the model weights were reinitialized.\n",
    "\n",
    "The team optimized the model using the Adam optimizer, setting parameters to β1 = 0.9, β2 = 0.999, with a learning rate of 1e-03. For the primary model, they processed 65,536 tokens per batch, distributed as 512 tokens across each of 128 GPUs. Each GPU handled a batch size of 8, leading to a total effective batch size of 1024. Training was completed over four days using 128 NVIDIA A100 GPUs, with DeepSpeed (Reference 69) managing model parallelism for efficient processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompting ProtGPT2 to generate sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the model for generation\n",
    "protgpt2 = pipeline('text-generation', model=\"nferruz/ProtGPT2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt 1\n",
    "sequences = protgpt2(\"<|endoftext|>MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERG\", max_length=100, do_sample=True, top_k=950, repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearning the sequences. \n",
    "for seq in sequences:\n",
    "    print(seq['generated_text'].strip('<|endoftext|>').replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Embedding from ProtGPT2\n",
    "\n",
    "Although ProtGPT2 is trained to generate sequence and it is a decoder only architechture, we can still retrieve embeddings learned from the trained dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name=\"nferruz/ProtGPT2\" ## link to hugging face repository\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name) # load the tokenizer\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name) # load the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a protein sequence, our aim is to get a fixed dimensional representation. In case protGPT2, it is 1280 dimensional vector learned for each token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence=\"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing the input sequence**\n",
    "\n",
    "It returns bunch of ids corresponding to the token dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens=tokenizer(sequence, return_tensors='pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id to Token mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id2tokens=[tokenizer.decode(iid) for iid in tokens['input_ids'][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(id2tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, tokens are variable length. These are learned using Byte-Pair Algorithm (BPE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(len(sequence),'-->', len(id2tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 110 amino acid long sequence is now tokenized into 26 tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing Token Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    outputs=model(**tokens, output_hidden_states=True)\n",
    "    #print(outputs)\n",
    "    hidden_states=outputs.hidden_states  ## all the hidden states output.\n",
    "    #print(hidden_states)\n",
    "    embeddings=hidden_states[-1]  ## embeddings of the last state. \n",
    "    #print(embeddings)\n",
    "    \n",
    "    embedding=embeddings.mean(dim=1) ## averaging embedding from all the tokens. \n",
    "    #print(len(embedding[0]),embedding[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets write a helper function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def embed_protein_sequence(sequence, model=model, tokenizer=tokenizer):\n",
    "    tokens=tokenizer(sequence, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs=model(**tokens, output_hidden_states=True)\n",
    "    \n",
    "        hidden_states=outputs.hidden_states  ## all the hidden states output.\n",
    "        \n",
    "        embeddings=hidden_states[-1]  ## embeddings of the last state. \n",
    "       \n",
    "        \n",
    "        embedding=embeddings.mean(dim=1) ## averaging embedding from all the tokens. \n",
    "        return embedding[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed_protein_sequence(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embed the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embdding the full dataset will take a lot of time. We prepared a another dataset with embedding taken from a public database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq_df=pd.read_csv('Seq_class.csv')\n",
    "seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequences=list(seq_df.sequence)\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes=list(seq_df.classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq_embeddings=[embed_protein_sequence(seq) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
