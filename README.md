# 2025 Data Science Symposium, South Dakota State University.

# Worskshop 1: Building Interdisciplinary applications using Large Language Models 
Held as part of  the 2025 Data Science Symposium at South Dakota State University, February 6-7, 2025, Brookings, SD. 
Tutorial Dates: February 6, 2025, at 13:00-17:00 hrs CST  
### Citing this tutorial
Please cite this tutorial as:

**Bishnu Sarker, Cameron Pykiet, Jaylin Dyson (2025, February). Workshop 1: Building Interdisciplinary applications using Large Language Models. In 2025 Data Science Symposium at South Dakota State University, February 6-7, 2025, Brookings, SD.**

### Overview
In the current decade, AI/ML has tremendously facilitated scientific discoveries in interdisciplinary research and development that includes agriculture, biomedicine and healthcare. Moreover, the recent advancements in the development of large language models (a type of deep learning model that can read, summarize, translate, and generate text as we humans do) have inspired many researchers to find applications in biological sequence analysis, partly because of the similarities in the data. Attention-based deep transformer models pre-trained in a self-supervised fashion on large corpus have dramatically transformed research in natural language processing. The attention mechanism involved in transformer models captures the long-distance relationship among words in textual data. Researchers have trained transformer-based language models for multi-disciplinary applications that includes agriculture, medicine, finance etc. They showed that transformer-based self-supervised language models effectively capture the spatial relationship in data which is critical for learning meaningful representation. 
In this half-day tutorial, we aim to provide experiential training on how to build machine learning pipelines using pre-trained transformer language models for interdisciplinary data science application. We will start with a quick introduction to Python packages (Pytorch, Scipy, scikit-learn) that are heavily used for machine learning projects. In addition, we will cover the domain knowledge behind individual applications. Then, self-supervised deep learning-based large language models (such as Transformers) will be reviewed with a particular focus on computational biology applications.  Finally, we will introduce SreamLit for creating web apps, and ollama package for connecting the LLMs. 



### Learning Objectives 
At the end of the tutorial, the participants will have understanding and practical knowledge of: 
1. Fundamentals of transformer-based large language models. 
3. How to build basic machine learning models for using large language models.  
5. How to apply a pre-trained transformer language model for data generation. 
7. How to formulate and address data science problems using transformer-based large language models. 
8. How to build web apps using Streamlit and large language models.


### Target Audience
The target audiences are graduate students, researchers, scientists, and practitioners in both academia and industry who are interested in applications of deep learning, natural language processing, and transformer-based language models in biomedicine and biomedical knowledge discovery. The tutorial is aimed towards entry-level participants with basic knowledge of computer programming (preferably Python) and machine learning (Beginner or Intermediate). 

### Instructions
The participants are requested to follow the following steps to prepare their work environment. 

#### Minimum Requirements:

- A computer
- High-speed internet. 
- A working python environment
- Minimum working knowledge of Python, particularly basic looping, lists, array, and tensors. A refresher on Python will be provided as a part of the tutorial. However, we recommend a quick refresher  on PyTorch and Scikit-Learn  Python packages for ML model development. 

#### Setting up the environment:
- Follow the instruction during the session.

# Outline of the Workshop

## DAY 1 - February, 2025 (13:00 -17:00 hrs CST)

### 13:00-13:50 hrs CST | Fundamentals of Large Language Models | [Slides]()
- Introduction to the tutorial session
- Fundamental concepts about Large Language Model. Brief theory and practical workflow.   

### 13:00-14:00 hrs CST  - 10 minutes Break/Q&A/Setting up worksapce
### 14:00-14:30 hrs CST | Introduction to App Development using StreamLit |
- Basic StreamLit Syntax
- App development using StreamLit

[Colab-Notebook-Python-Refresher](https://github.com/Bishnukuet/ISMB_ECCB_2023_VT2_LLM/blob/main/notebooks/Day%201-Part-1A-Python-Refresher.ipynb)

### 14:30-15:00 hrs CST  - 3.	Hands-On tutorial  on Text Classification using LLM and StreamLit. 

### 15:00-15:10 hrs CST  - 10 minutes Break/Q&A/Setting up worksapce

### 15:10-16:00 hrs CEST - 3.	Hands-On tutorial on Retrieval Augmented Generation (RAG) using LLM and StreamLit. 


### 16:10-16:45 hrs CEST | Large Language Model for Protein Sequence Generation and Analysis. 

### 16:45-17:00 hrs CEST  - 15 Concluding Remarks and Q&A
