# 2025 Data Science Symposium, South Dakota State University.

# Worskshop 1: Building Interdisciplinary applications using Large Language Models 
Held as part of  the 2025 Data Science Symposium at South Dakota State University, February 6-7, 2025, Brookings, SD. 
Tutorial Dates: February 6, 2025, at 13:00-17:00 hrs CST  
### Citing this tutorial
Please cite this tutorial as:

**Bishnu Sarker, Cameron Pykiet, Jaylin Dyson (2025, February). Workshop 1: Building Interdisciplinary applications using Large Language Models. In 2025 Data Science Symposium at South Dakota State University, February 6-7, 2025, Brookings, SD.**

### Overview
In the current decade, AI/ML has tremendously facilitated scientific discoveries in interdisciplinary research and development that includes agriculture, biomedicine and healthcare. Moreover, the recent advancements in the development of large language models (a type of deep learning model that can read, summarize, translate, and generate text as we humans do) have inspired many researchers to find applications in biological sequence analysis, partly because of the similarities in the data. Attention-based deep transformer models pre-trained in a self-supervised fashion on large corpus have dramatically transformed research in natural language processing. The attention mechanism involved in transformer models captures the long-distance relationship among words in textual data. Researchers have trained transformer-based language models for multi-disciplinary applications that includes agriculture, medicine, finance etc. They showed that transformer-based self-supervised language models effectively capture the spatial relationship in data which is critical for learning meaningful representation. 
In this half-day tutorial, we aim to provide experiential training on how to build machine learning pipelines using pre-trained transformer language models for interdisciplinary data science application. We will start with a quick introduction to Python packages (Pytorch, Scipy, scikit-learn) that are heavily used for machine learning projects. In addition, we will cover the domain knowledge behind individual applications. Then, self-supervised deep learning-based large language models (such as Transformers) will be reviewed with a particular focus on computational biology applications.  Finally, we will introduce SreamLit for creating web apps, and ollama package for connecting the LLMs. 



### Learning Objectives 
At the end of the tutorial, the participants will have understanding and practical knowledge of: 
1. Fundamentals of transformer-based large language models. 
3. How to build basic machine learning models for using large language models.  
5. How to apply a pre-trained transformer language model for data generation. 
7. How to formulate and address data science problems using transformer-based large language models. 
8. How to build web apps using Streamlit and large language models.


### Target Audience
The target audiences are graduate students, researchers, scientists, and practitioners in both academia and industry who are interested in applications of deep learning, natural language processing, and transformer-based language models in biomedicine and biomedical knowledge discovery. The tutorial is aimed towards entry-level participants with basic knowledge of computer programming (preferably Python) and machine learning (Beginner or Intermediate). 

### Instructions
The participants are requested to follow the following steps to prepare their work environment. 

#### Minimum Requirements:

- A computer
- High-speed internet. 
- A working python environment
- Minimum working knowledge of Python, particularly basic looping, lists, array, and tensors. A refresher on Python will be provided as a part of the tutorial. However, we recommend a quick refresher  on PyTorch and Scikit-Learn  Python packages for ML model development. 

#### Setting up the environment:
- Follow the instruction during the session.

# Outline of the Workshop

### Session 1: 13:00-13:50 hrs CST | Fundamentals of Large Language Models | [Slides]()
- Introduction to the tutorial session
- Fundamental concepts about Large Language Model. Brief theory and practical workflow.   

### 13:00-14:00 hrs CST  - 10 minutes Break/Q&A/Setting up worksapce

### Session 2: 14:00-14:30 hrs CST | Introduction to App Development using StreamLit | [Notebook](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-WorkShop1/blob/53bf21514aad0460d1639a67c877a8fc985980f0/1_Case_study_Streamlit.ipynb) 
- Basic StreamLit Syntax
- App development using StreamLit


### Session 3: 14:30-15:00 hrs CST  - Hands-On tutorial  on Text Classification using LLM and StreamLit | [Notebook](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-WorkShop1/blob/4058a7abe41bcc518fb091784e56845c55605549/2_Case_study_sentiment.ipynb) 
- Building a sentiment classification model using locally installed LLMs.
- Building an user interface using StreamLit. 

### 15:00-15:10 hrs CST  - 10 minutes Break/Q&A/Setting up worksapce

### Session 4: 15:10-16:00 hrs CEST - Hands-On tutorial on Retrieval Augmented Generation (RAG) using LLM and StreamLit | [notebook](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-WorkShop1/blob/4058a7abe41bcc518fb091784e56845c55605549/03_Case_study_RAG.ipynb)
- Understanding the Basic Concepts of RAG.
- Implementing a RAG architecture using locally installed LLM.
- Building an user interface using StreamLit. 

### Session 5: 16:10-16:45 hrs CST | Large Language Model for Protein Sequence Generation and Analysis | [Slides](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-Workshop/blob/37729940d4a76a35140f2eb0c751c5415663a305/Presentation%202-Case-Study-Protein-Function-Prediction.pdf) | [notebook 1](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-Workshop/blob/37729940d4a76a35140f2eb0c751c5415663a305/4-Hands-On-Tutorial-On-Loading-Pre-Trainied-Transformers-From-HF.ipynb) | [Notebook 2](https://github.com/Pykietcj11/2025-SDSU-Data-Science-Symposium-Workshop/blob/37729940d4a76a35140f2eb0c751c5415663a305/5-Hands-On-Tutorial-On-Protein-Function-Prediction-ProTrans.ipynb)
- Understanding the interdisciplinary application of LLMs.
- Building an machine learning modeling using protein language model.
- Demonstrating LLMs for sequence generation. 

### 16:45-17:00 hrs CST  - 15 Concluding Remarks and Q&A

# Speakers

## 1. Cameron Pykiet
Cameron Pykiet is a second year MS in Data Science student in the School of Applied Computational Sciences at Meharry Medical College. He is an active member of Sarker’s lab Omics Mining and Algorithmic Reasoning (L’OMAR).  His research focus is in building a large language model with new tokenization for protein function prediction. He has expertise in building and using large language model from scratch. 

## 2. Jaylin Dyson
Jaylin Dyson is a second year MS in Data Science student in the School of Applied Computational Sciences at Meharry Medical College. He is an active member of Sarker’s lab Omics Mining and Algorithmic Reasoning (L’OMAR).  His research focus is in building a deep learning framework for knowledge graph based protein function prediction . He has expertise in building and using knowledge graphs for biomedical knowledge discovery.   

## 3. Bishnu Sarker 
Bishnu Sarker is an Assistant Professor of Computer Science and Data Science at Meharry Medical College, Nashville, TN, USA. He is the Principal Investigator of Sarker’s lab Omics Mining and Algorithmic Reasoning (L’OMAR). His research focus is on applying AI, deep learning, natural language processing (NLP), and graph-based reasoning approaches to effectively describe proteins numerically and to infer their functional characteristics from complex, heterogeneous, and interconnected biomedical data. He received his BS from Khulna University of Engineering and Technology, Bangladesh; MS from Sorbonne University, France; and PhD from INRIA, France. During his PhD he spent a winter at MILA - Quebec AI Institute and University of Montreal, Canada, as a visiting researcher with the DrEAM mobility grant from University of Lorraine. 


# Acknoledgement
This material is based upon work partially supported by the National Science Foundation under Grant No. 2302637. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
Additionally, we would like to acknowldge the travel support from South Dakota State University and Meharry Medical College. 

