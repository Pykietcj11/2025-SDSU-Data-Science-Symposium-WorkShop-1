{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb4bd1f",
   "metadata": {},
   "source": [
    "### **2025 SDSU Data Science Symposium\n",
    "### South Dakota State University, Brookings SD**\n",
    "\n",
    "**Date of Session:** February 6, 2025 \n",
    "\n",
    "**Instructor(s):** Cameron Pykiet, Jaylin Dyson, Bishnu Sarker\n",
    "\n",
    "**Affiliation:** Meharry Medical College School of Applied Computational Sciences, Tennessee, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6908f",
   "metadata": {},
   "source": [
    "# Using Streamlit with Ollama for sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb29bdb",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "For this section of the tutorial, we will build a simple web-based NLP application to analyize reviews for Sentiment Analysis. We will make use of a Large Language Model (LLM) from the **Ollama** package, and create a user interface with **Streamlit**. \n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Prerequisites**\n",
    "- **Python 3.7+**\n",
    "- Basic knowledge of Python Programming\n",
    "- Install **Ollama & Streamlit**\n",
    "\n",
    "        !pip install ollama streamlit\n",
    "    \n",
    "--- \n",
    "\n",
    "### 2. a) Ollama Installation \n",
    "\n",
    "Follow the instructions below:\n",
    "\n",
    "Download and install Ollam from the following web: https://ollama.com/ . Click on download button and follow the steps according to your OS (MC or PC).\n",
    "Install the by clicking the downloaded software.\n",
    "\n",
    "Alternatively, you can also look here for source code of ollama: https://github.com/ollama/ollama \n",
    "\n",
    "We will be utilizing the llama3.2 model in this demonstration.\n",
    "This specific llama model is from Meta and has a size of 3B parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. b) Ollama testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce5aef52",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /opt/anaconda3/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.11/site-packages (1.39.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: entrypoints in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce4cf931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ollama to make sure it works\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "957eb5c1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'spam_Ollama:latest',\n",
       "   'model': 'spam_Ollama:latest',\n",
       "   'modified_at': '2024-12-05T16:18:29.756616099-06:00',\n",
       "   'size': 807691078,\n",
       "   'digest': 'a5aa4550ffc57110ae2c2c2ae3f1108eb8bfdc938d552f93de7bd8ad0ba10d07',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '1.2B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'llama3.2:latest',\n",
       "   'model': 'llama3.2:latest',\n",
       "   'modified_at': '2024-10-16T19:26:18.135739959-05:00',\n",
       "   'size': 2019393189,\n",
       "   'digest': 'a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '3.2B',\n",
       "    'quantization_level': 'Q4_K_M'}}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38a01f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2', 'created_at': '2025-01-30T17:51:22.690776Z', 'message': {'role': 'assistant', 'content': \"A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's trained on vast amounts of text data, which enables it to learn patterns, relationships, and context. LLMs use complex algorithms to generate responses, answers, or text based on the input they receive. They can be used for tasks such as language translation, text summarization, question answering, and more. LLMs have become increasingly popular in areas like natural language processing (NLP), chatbots, and virtual assistants due to their ability to understand and respond to human-like language inputs.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 9845562292, 'load_duration': 816609125, 'prompt_eval_count': 37, 'prompt_eval_duration': 4087000000, 'eval_count': 127, 'eval_duration': 4937000000}\n"
     ]
    }
   ],
   "source": [
    "## To chat with llma3.2\n",
    "prompt={'role':'user', 'content':\"With approximately 100 words explain what is large language model?\"}\n",
    "llm='llama3.2'\n",
    "response = ollama.chat(model=llm, messages=[prompt])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a125eb4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a type of artificial intelligence (AI) designed to process and understand human language. It's trained on vast amounts of text data, which enables it to learn patterns, relationships, and context. LLMs use complex algorithms to generate responses, answers, or text based on the input they receive. They can be used for tasks such as language translation, text summarization, question answering, and more. LLMs have become increasingly popular in areas like natural language processing (NLP), chatbots, and virtual assistants due to their ability to understand and respond to human-like language inputs.\n"
     ]
    }
   ],
   "source": [
    "answer = response['message']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050c1f7",
   "metadata": {},
   "source": [
    "#### **Explaination** \n",
    "- prompt is a dictionary with two parts: 'role' and 'content'\n",
    "- Using `ollama.chat()` allows for chatting with the model using prompt.\n",
    "- Access the `.chat()` text response via message content as show above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51b3a6",
   "metadata": {},
   "source": [
    "### 3. Setting up Streamlit Application\n",
    "\n",
    "    1. Create folder or use desired directory\n",
    "    2. Create '4_Sentiment_app.py' within folder/directory\n",
    "        Creating this file in jupyter is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82d648",
   "metadata": {},
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d146fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 4_sentiment_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import ollama\n",
    "\n",
    "st.title('Ollama Sentiment Analysis')\n",
    "st.caption('''\n",
    "    This application is used to perform sentiment analysis \n",
    "    on input text data and will score the input as \"Positive\",\n",
    "    \"Negative\", or \"Neutral.\"\n",
    "    ''')\n",
    "\n",
    "llm='llama3.2'\n",
    "user_input = str(st.chat_input('Place text for review here:'))\n",
    "\n",
    "response=ollama.generate(model=llm, prompt=\"Respond only with 'Positive' or 'Negative' or 'Neutral' for the review:\"+user_input)\n",
    "\n",
    "if user_input:\n",
    "    contents=response['response']\n",
    "    st.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d222225",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- `st.chat_input()` Used to create a chat box for input text that will serve as the part of the prompt to the ollama LLM. \n",
    "- `prompt` We will be using pre-prompting for the ollama llm to perform sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "975749fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pages/4_sentiment_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Sidebar for selecting application mode\n",
    "page = st.sidebar.selectbox(\n",
    "    \"Choose a Mode\",\n",
    "    [\"Ollama Chat\", \"Sentiment Analysis\"]\n",
    ")\n",
    "\n",
    "if page == \"Ollama Chat\":\n",
    "    st.title(\"Ollama Chat\")\n",
    "    st.caption(\"A simple chatbot using Ollama.\")\n",
    "\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "\n",
    "    user_input = st.text_input(\"Enter your message:\")\n",
    "    \n",
    "    if user_input:\n",
    "        response = ollama.generate(model=\"llama3.2\", prompt=user_input)\n",
    "        if 'response' in response:\n",
    "            st.session_state.chat_history.append((\"You\", user_input))\n",
    "            st.session_state.chat_history.append((\"Ollama\", response['response']))\n",
    "    \n",
    "    for sender, msg in st.session_state.chat_history:\n",
    "        st.write(f\"**{sender}:** {msg}\")\n",
    "\n",
    "\n",
    "elif page == \"Sentiment Analysis\":\n",
    "    st.title(\"Ollama Sentiment Analysis\")\n",
    "    st.caption(\n",
    "        \"Enter text to classify sentiment as 'Positive', 'Negative', or 'Neutral'.\\n\\n\"\n",
    "        \"This works by using a prompt-based approach with Ollama, where the model is \"\n",
    "        \"instructed to strictly respond with one of the three sentiment labels. \"\n",
    "        \"By carefully crafting the prompt, we guide the model to provide structured \"\n",
    "        \"and predictable responses, ensuring consistency in sentiment classification.\"\n",
    "    )\n",
    "\n",
    "    analysis_mode = st.radio(\"Choose Input Mode:\", [\"Single Review\", \"Batch Processing (CSV)\"])\n",
    "\n",
    "    if analysis_mode == \"Single Review\":\n",
    "        user_input = st.chat_input(\"Place text for review here:\")\n",
    "\n",
    "        if user_input:\n",
    "            sentiment_prompt = f\"Classify the following review as 'Positive', 'Negative', or 'Neutral': {user_input}\"\n",
    "            response = ollama.generate(model=\"llama3.2\", prompt=sentiment_prompt)\n",
    "\n",
    "            if 'response' in response:\n",
    "                st.write(f\"**Sentiment:** {response['response']}\")\n",
    "\n",
    "    elif analysis_mode == \"Batch Processing (CSV)\":\n",
    "        uploaded_file = st.file_uploader(\"Upload a CSV file with a column named 'review'\", type=[\"csv\"])\n",
    "\n",
    "        if uploaded_file is not None:\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "\n",
    "            if \"review\" not in df.columns:\n",
    "                st.error(\"The uploaded CSV must have a column named 'review'.\")\n",
    "            else:\n",
    "                st.write(\"Processing reviews... This may take some time.\")\n",
    "\n",
    "                sentiments = []\n",
    "                for review in df[\"review\"]:\n",
    "                    sentiment_prompt = f\"Classify the following review as 'Positive', 'Negative', or 'Neutral': {review}\"\n",
    "                    response = ollama.generate(model=\"llama3.2\", prompt=sentiment_prompt)\n",
    "                    sentiments.append(response.get(\"response\", \"Unknown\"))\n",
    "\n",
    "                df[\"Sentiment\"] = sentiments\n",
    "\n",
    "                # Display results\n",
    "                st.dataframe(df)\n",
    "\n",
    "                # Create pie chart\n",
    "                sentiment_counts = df[\"Sentiment\"].value_counts()\n",
    "                fig = px.pie(\n",
    "                    names=sentiment_counts.index,\n",
    "                    values=sentiment_counts.values,\n",
    "                    title=\"Sentiment Distribution\",\n",
    "                    color=sentiment_counts.index,\n",
    "                    color_discrete_map={\"Positive\": \"green\", \"Negative\": \"red\", \"Neutral\": \"gray\"},\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c701e4f",
   "metadata": {},
   "source": [
    "#### Explaination:\n",
    "- `st.sidebar.selectbox()` Creates a selectbox within the sidebar when toggled. \n",
    "- `st.session_state.chat_history()` Allows for all prior messages to be saved in one \n",
    "- `st.error()` Handles errors and gives a response when an error occurs\n",
    "- `st.plotly_chart()` Using plotly to create a pie chart for the given "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae10ac1",
   "metadata": {},
   "source": [
    "!streamlit run 4_sentiment_app.py\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial, we demonstrated how to utilize a **ollama** LLM and how to build a web application through **streamlit**. We were able to build a web interface that utilizes **ollama** for a simple chat bot as well as a sentiment analysis application with **ollama** by using specific prompting. This could work on either a single review or a batch of reviews in a `.csv` file. This is just one example of how a small LLM can be utilized. Feel free to explore more features, models, and applications of LLMs as you continue building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c1489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
